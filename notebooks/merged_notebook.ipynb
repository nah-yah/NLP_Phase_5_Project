{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f13e1af",
   "metadata": {},
   "source": [
    "# NLP-based classification of haitian disaster response messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526adbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca152ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "messages = pd.read_csv(\"disaster_messages.csv\")\n",
    "print(\"Messages columns:\", messages.columns.tolist())\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cd2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.read_csv(\"disaster_categories.csv\")\n",
    "print(\"Categories columns:\", cat.columns.tolist())\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a00a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge datasets\n",
    "df = messages.merge(cat, left_on='id', right_on='id', how='inner')\n",
    "df.head()\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d52fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse categories\n",
    "categories = df[\"categories\"].str.split(';', expand=True)\n",
    "category_col = categories.iloc[0].str.split('-').str[0].tolist()\n",
    "\n",
    "categories.columns = category_col\n",
    "for column in categories:\n",
    "    categories[column] = categories[column].str[-1]\n",
    "    \n",
    "    categories[column] = pd.to_numeric(categories[column])\n",
    "\n",
    "df.drop(['categories'], axis=1, inplace = True)\n",
    "\n",
    "# concatenate\n",
    "df = pd.concat([df, categories], axis=1)\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caec6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "haiti_mask = (\n",
    "    (df['genre'] == 'direct') |\n",
    "    (df['original'].str.contains('Haiti', case=False, na=False)) |\n",
    "    (df['message'].str.contains('Haiti', case=False, na=False))\n",
    ")\n",
    "df = df[haiti_mask].copy()\n",
    "\n",
    "# disaster-related messages\n",
    "df = df[df[\"related\"] == 1].reset_index(drop=True)\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates(subset=['message', 'genre']).reset_index(drop=True)\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e6a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (df[\"request\"] == 1),\n",
    "    (df[\"request\"] == 0)\n",
    "]\n",
    "choices = [\"request\", \"info\"]\n",
    "df[\"target\"] = np.select(conditions, choices, default=\"info\")\n",
    "\n",
    "target_percentages = df[\"target\"].value_counts(normalize=True) * 100\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a76cb4",
   "metadata": {},
   "source": [
    "- conversion to lowercase\n",
    "- remove URLs, social media mentions, hashtags, punctuation, ...\n",
    "- tokenization into words\n",
    "- filtering of English stopwords \n",
    "- fltering of non-alphabetic tokens\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23296abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "    # remove URLs etc..\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords and non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stopwords.words('english')]\n",
    "    # lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean_message'] = df['message'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6146c96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['clean_message'].str.split().str.len()\n",
    "df['char_count'] = df['message'].str.len()\n",
    "\n",
    "print(\"Word count\", df['word_count'].mean())\n",
    "print(\"Message length\", df['char_count'].min(), \"â€“\", df['char_count'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ac616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test (.8/.2)\n",
    "X = df['clean_message']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7217dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "order = ['request', 'info']\n",
    "counts = df['target'].value_counts().reindex(order, fill_value=0)\n",
    "\n",
    "ax = sns.barplot(x=counts.index, y=counts.values)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Amee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31119d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(category, n=10):\n",
    "    subset = df[df['target'] == category]['clean_message']\n",
    "    vec = CountVectorizer(max_features=1000, stop_words='english')\n",
    "    X = vec.fit_transform(subset)\n",
    "    freq = np.array(X.sum(axis=0)).flatten()\n",
    "    words = vec.get_feature_names_out()\n",
    "    top_idx = freq.argsort()[-n:][::-1]\n",
    "    return [(words[i], freq[i]) for i in top_idx]\n",
    "\n",
    "def plot_wordcloud(text, title, ax):\n",
    "    wordcloud = WordCloud(\n",
    "        width=400,\n",
    "        height=300,\n",
    "        background_color='white',\n",
    "        colormap='viridis',\n",
    "        max_words=100,\n",
    "        stopwords=set(stopwords.words('english'))\n",
    "    ).generate(text)\n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "request_text = ' '.join(df[df['target'] == 'request']['clean_message'])\n",
    "info_text = ' '.join(df[df['target'] == 'info']['clean_message'])\n",
    "\n",
    "# plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "plot_wordcloud(request_text, 'Requests', axes[0])\n",
    "plot_wordcloud(info_text, 'Info', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Amee\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines\n",
    "def make_pipeline(clf):\n",
    "    return Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            max_features=7000,\n",
    "            ngram_range=(1, 3),\n",
    "            sublinear_tf=True,\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            stop_words='english'\n",
    "        )),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "models = {\n",
    "    'LR': make_pipeline(LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)),\n",
    "    'Naive Bayes': make_pipeline(MultinomialNB()),\n",
    "    'Random Forest': make_pipeline(RandomForestClassifier(class_weight='balanced', random_state=42))\n",
    "}\n",
    "\n",
    "# train models\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560d908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 7000],\n",
    "    'tfidf__ngram_range': [(1, 3)],\n",
    "    'clf__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    models['LR'],\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "\n",
    "# best estimator\n",
    "best_lr = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c44a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models.copy()\n",
    "models['Tuned LR'] = best_lr\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    y_test_int = (y_test == 'request').astype(int)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "    auc = roc_auc_score(y_test_int, y_proba)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': round(acc, 5),\n",
    "        'Macro F1': round(f1_macro, 5)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned = best_lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_tuned, labels=['request', 'info'])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['request', 'info'],\n",
    "    yticklabels=['request', 'info']\n",
    ")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f077393",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = best_lr.predict_proba(X_test)\n",
    "\n",
    "label_map = {'info': 0, 'request': 1}\n",
    "y_test_int = y_test.map(label_map).values\n",
    "y_proba_positive = y_proba[:, 1]\n",
    "\n",
    "# ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test_int, y_proba_positive)\n",
    "print(f\"ROC-AUC (binary): {roc_auc:.3f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test_int, y_proba_positive)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
